\section{Optimization Methods}

In order to explain the \emph{convergence} and \emph{efficiency} properties of the following optimization methods, we need to introduce some preliminary definitions about \emph{convexity} and the \emph{L-Lipschitz continuity} of a function~\cite{boyd2004convex}.

\begin{definition}[Convexity] \label{def:convexity}
	\hfill
	\begin{enumerate}[(i)]
		\item We say that a function $f: \Re^m \rightarrow \Re$ is convex if: 
		$$ 
			(\lambda x + (1 - \lambda) y ) \leq \lambda f(x) + (1 - \lambda) f(y) \ \forall \ x, y \in \Re^m, \lambda \in [0,1] 
		$$
		\item We say that a differentiable function $f: \Re^m \rightarrow \Re$, i.e., $f \in C^1$, is convex if: 
		$$ 
			f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle \ \forall \ x, y \in \Re^m 
		$$
		\item We say that a twice differentiable function $f: \Re^m \rightarrow \Re$, i.e., $f \in C^2$ and the Hessian matrix is symmetric, is convex iff: 
		$$ 
			\nabla^2 f(x) \succeq 0 \ \forall \ x \in \Re^m 
		$$ i.e., the Hessian matix is \emph{positive semidefinite}.
	\end{enumerate}
\end{definition}

\begin{definition}[Strict Convexity] \label{def:strict_convexity}
	\hfill
	\begin{enumerate}[(i)]
		\item We say that a function $f: \Re^m \rightarrow \Re$ is strictly convex if: 
		$$ 
			(\lambda x + (1 - \lambda) y ) < \lambda f(x) + (1 - \lambda) f(y) \ \forall \ x, y \in \Re^m, x \neq y, \lambda \in (0,1) 
		$$
		\item We say that a differentiable function $f: \Re^m \rightarrow \Re$, i.e., $f \in C^1$, is strictly convex if: 
		$$ 
			f(y) > f(x) + \langle \nabla f(x), y - x \rangle \ \forall \ x, y \in \Re^m, x \neq y
		$$
		\item We say that a twice differentiable function $f: \Re^m \rightarrow \Re$, i.e., $f \in C^2$ and the Hessian matrix is symmetric, is strictly convex iff: 
		$$ 
			\nabla^2 f(x) \succ 0 \ \forall \ x \in \Re^m 
		$$ i.e., the Hessian matix is \emph{positive definite}.
	\end{enumerate}
\end{definition}

\begin{definition}[Strong Convexity] \label{def:strong_convexity}
We say that a function $f: \Re^m \rightarrow \Re$ is $\mu$-strongly convex if the function:
$$
g(x) = f(x) - \frac{\mu}{2} \| x \|^2
$$
is convex for any $\mu > 0$. 
If $f$ is differentiable, i.e., $f \in C^1$, this is also equivalent to:
$$
f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle + \frac{\mu}{2} \| y - x \|^2 \ \forall \ x, y \in \Re^m
$$
and, if $f$ is a twice differentiable function, i.e., $f \in C^2$ and the Hessian matrix is symmetric, then $f$ is $\mu$-strongly convex iff:
$$
\nabla^2 g(x) \succ 0 \ \forall \ x \in \Re^m
$$
i.e., the Hessian matix is \emph{positive definite}, which is:
$$
\nabla^2 f(x) \succeq \mu I \ \forall \ x \in \Re^m
$$
\end{definition}

\begin{definition}[L-Lipschitz continuity] \label{def:l_lipschitz_continuity}
We say that a function $f: \Re^m \rightarrow \Re$ is L-smooth, i.e., L-Lipschitz continuous, if:
$$
\| \nabla f(x) - \nabla f(y) \| \leq L \| x - y \| \ \forall \ x, y \in \Re^m
$$
then:
$$
f(y) \leq f(x) + \langle \nabla f(x), y - x \rangle + \frac{L}{2} \| y - x \|^2 \ \forall \ x, y \in \Re^m
$$
and, if $f$ is a $\mu$-strongly convex function, we give the following Hessian bounds:
$$
0 \prec \mu I \preceq \nabla^2 f(x) \preceq L I \ \forall \ x \in \Re^m
$$
We say that a function $f: \Re^m \rightarrow \Re$ is locally L-smooth, i.e., locally L-Lipschitz continuous, if for every $x$ in $\Re^m$ there exists a neighborhood $U$ of $x$ such that $f$ restricted to $U$ is L-Lipschitz continuous. Every convex function is locally L-Lipschitz continuous.
\end{definition}

\begin{definition}[Subgradient] \label{def:subgradient}
Given a function $f: \Re^m \rightarrow \Re$ and $x \in \Re^m$, we define a subgradient $g \in \Re^m$ at $x$ to be any point satisfying:
$$
	f(y) \geq f(x) + \langle g, y - x \rangle \ \forall \ y \in \Re^m
$$
Subgradients always exist for convex function.
\end{definition}

\pagebreak

\subsection{Gradient Descent for Primal formulations}

The Gradient Descent algorithm is the simplest \emph{first-order optimization} method that exploits the orthogonality of the gradient wrt the level sets to take a descent direction. In particular, it performs the following iterations:

\begin{algorithm}[H]
	\caption{Gradient Descent}
	\label{alg:gd}
	\begin{algorithmic}
		\Require{Function $f$ to minimize}
		\Require{Learning rate or step size $\alpha > 0$}
		\Function{GradientDescent}{$f,\alpha$}
			\State Initialize weight vector $x_0$
			\State $t = 0$
			\While{$not\_convergence$}
				\State $x_{t+1} = x_t - \alpha \partial f(x_t)$ \Comment if $f$ is differentiable then $\partial f(x_t) = \nabla f(x_t)$
				\State $t = t + 1$
			\EndWhile
			\State \Return $x_t$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Gradient Descent is based on full gradients, since at each iteration we compute the average gradient on the whole dataset:
$$
\partial f(x) = \frac{1}{n} \sum_{i=1}^n \partial f_i(x)
$$
The downside is that every step is very computationally expensive, $\mathcal{O}(nm)$ per iteration, where $n$ is the number of samples in our dataset and $m$ is the number of dimensions.

Since \emph{Gradient Descent} becomes impractical when dealing with large datasets we introduce a stochastic version, called \emph{Stochastic Gradient Descent}, which does not use the whole set of examples to compute the gradient at every step. By doing so, we can reduce computation all the way down to $\mathcal{O}(m)$ per iteration.

\begin{algorithm}[H]
	\caption{Stochastic Gradient Descent}
	\label{alg:sgd}
	\begin{algorithmic}
		\Require{Function $f$ to minimize}
		\Require{Learning rate or step size $\alpha > 0$}
		\Require{Batch size $k$}
		\Function{StochasticGradientDescent}{$f,\alpha,k$}
			\State Initialize weight vector $x_0$
			\State $t \gets 0$
			\While{$not\_convergence$}
				\State Sample $(i_1,\dots,i_k) \sim \mathcal{U}^k(1,\dots,n)$ 
				\State $\displaystyle x_{t+1} \gets x_t - \alpha \frac{1}{k} \sum_{j=1}^k \partial f_{i_j}(x_t)$ \Comment if $f$ is differentiable then $\partial f_{i_j}(x_t) = \nabla f_{i_j}(x_t)$
				\State $t \gets t + 1$
			\EndWhile
			\State \Return $x_t$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Note that in expectation, we converge like GD, since $\displaystyle \mathbb{E}_{i \sim \mathcal{U}(1,\dots,n)}[\partial f_i(x_t)] = \partial f(x_t)$, therefore, the expected iterate of SGD converges to the optimum.

% https://erwanscornet.github.io/teaching/Optimization.pdf
% https://www.math.univ-toulouse.fr/~agarivie/sites/default/files/8_optimization.pdf

Now, consider the SGD algorithm introduced previously but where each iteration is projected into the ball $\mathcal{B}(0, R)$ with radius $R > 0$ fixed. So, the following lower bounds on convergence rates are given.

\begin{theorem}[Stochastic Gradient Descent convergence for convex functions] \label{thm:cvx_sgd_convergence}
Let $f: \Re^m \rightarrow \Re$ be a L-Lipschitz continuous convex function and assume that exists $b > 0$ satisfying:
$$
\| f_i(x) \| \leq b \ \forall \ x \in \mathcal{B}(0, R)
$$
Besides, assume that all minima of $f$ belong to $\mathcal{B}(0, R)$. Then the Stochastic Gradient Descent with step size $\displaystyle \alpha = \frac{2R}{b\sqrt{k}}$ satisfies:
$$
\mathbb{E}\Bigg[f\Bigg(\frac{1}{k} \sum_{t=1}^k x_t\Bigg)\Bigg] - f(x^*) \leq \frac{3Rb}{\sqrt{k}}
$$
\end{theorem}

\begin{theorem}[Stochastic Gradient Descent convergence for strongly convex functions] \label{thm:str_cvx_sgd_convergence}
Let $f: \Re^m \rightarrow \Re$ be a L-Lipschitz continuous, $\mu$-strongly convex function and assume that exists $b > 0$ satisfying:
$$
\| f_i(x) \| \leq b \ \forall \ x \in \mathcal{B}(0, R)
$$
Besides, assume that all minima of $f$ belong to $\mathcal{B}(0, R)$. Then the Stochastic Gradient Descent with step size $\displaystyle \alpha = \frac{2}{\mu(k+1)}$ satisfies:
$$
\mathbb{E}\Bigg[f\Bigg(\frac{2}{k(k+1)} \sum_{t=1}^k t x_{t-1}\Bigg)\Bigg] - f(x^*) \leq \frac{2b^2}{\mu(k+1)}
$$
\end{theorem}

SGD’s \emph{convergence rate} for L-Lipschitz continuous convex functions is $\displaystyle \mathcal{O}\bigg(\frac{1}{\sqrt{t}}\bigg)$ and $\displaystyle \mathcal{O}\bigg(\frac{1}{t}\bigg)$ for L-Lipschitz continuous and strongly convex functions. More iterations are needed to reach the same accuracy as GD, but the iterations are far cheaper.

% http://www.princeton.edu/~yc5/ele522_optimization/lectures/subgradient_methods.pdf

\subsubsection{Nonsmooth}

First, consider a nonsmooth, i.e., nondifferentiable, convex function. So, the following lower bounds on convergence rates are given.

\begin{theorem}[Subgradient Descent convergence for convex functions with Polyak's stepsize] \label{thm:cvx_polyak_subgd_convergence}
Let $f: \Re^m \rightarrow \Re$ be a L-Lipschitz continuous convex function. Then the Subgradient Descent with Polyak's step size $\displaystyle \alpha_t = \frac{f(x_t) - f(x^*)}{\| g_t \|^2}$ satisfies:
$$
f(x_t) - f(x^*) \leq \frac{L \| x_0 - x^* \|^2}{\sqrt{t+1}}
$$
\end{theorem}

Unfortunately, Polyak’s stepsize rule requires knowledge of $f(x^*)$, which is often unknown a priori, so we might often need simpler rule for setting stepsizes.

\begin{theorem}[Subgradient Descent convergence for convex functions] \label{thm:cvx_subgd_convergence}
Let $f: \Re^m \rightarrow \Re$ be a L-Lipschitz continuous convex function. Then the Subgradient Descent with step size $\displaystyle \alpha_t = \frac{1}{\sqrt{t}}$ satisfies:
$$
f(x_t) - f(x^*) \leq \frac{\| x_0 - x^* \|^2 + L^2 \log t}{\sqrt{t}}
$$
\end{theorem}

\begin{theorem}[Subgradient Descent convergence for strongly convex functions] \label{thm:str_cvx_subgd_convergence}
Let $f: \Re^m \rightarrow \Re$ be a L-Lipschitz continuous and $\mu$-strongly convex function. Then the Subgradient Descent with step size $\displaystyle \alpha_t = \frac{2}{\mu(t+1)}$ satisfies:
$$
f(x_t) - f(x^*) \leq \frac{2L^2}{\mu} \frac{1}{t+1}
$$
\end{theorem}

The Subgradient Descent \emph{convergence rate} for L-Lipschitz continuous convex functions is $\displaystyle \mathcal{O}\Bigg(\frac{1}{\sqrt{t}}\Bigg)$ and $\displaystyle \mathcal{O}\Bigg(\frac{1}{t}\Bigg)$ for L-Lipschitz continuous and strongly convex functions. We can also  write the \emph{iteration complexity}, i.e., the smallest $t$ such that we’re within $\epsilon$-close to global optimum, as $\displaystyle \mathcal{O}\Bigg(\frac{1}{\epsilon^2}\Bigg)$ for L-Lipschitz continuous convex functions and as $\displaystyle \mathcal{O}\Bigg(\frac{1}{\epsilon}\Bigg)$ for L-Lipschitz continuous and strongly convex functions.

Among algorithms that only use subgradient, these \emph{convergence rates} cannot be futher improved.

% http://www.stat.cmu.edu/~larry/=sml/optrates.pdf

\subsubsection{Smooth}

Now, consider a smooth, i.e., differentiable, convex function. So, the following lower bounds on convergence rates are given.

\begin{theorem}[Gradient Descent convergence for convex functions] \label{thm:cvx_gd_convergence}
Let $f: \Re^m \rightarrow \Re$ be a L-Lipschitz continuous convex function. Then the Gradient Descent with step size $\alpha \leq 1/L$ satisfies:
$$
f(x_t) - f(x^*) \leq \frac{\| x_0 - x^* \|^2}{2 \alpha t}
$$
In particular, for $\alpha = 1/L$:
$$
f(x_t) - f(x^*) \leq \frac{L \| x_0 - x^* \|^2}{2 t}
$$
\end{theorem}

\begin{theorem}[Gradient Descent convergence for strongly convex functions] \label{thm:str_cvx_gd_convergence}
Let $f: \Re^m \rightarrow \Re$ be a L-Lipschitz continuous and $\mu$-strongly convex function. Then the Gradient Descent with step size $\alpha \leq 1/L$ satisfies:
$$
f(x_t) - f(x^*) \leq (1 - \alpha \mu)^t \| x_0 - x^* \|^2
$$
In particular, for $\alpha = 1/L$:
$$
\begin{aligned}
	f(x_t) - f(x^*) \leq & \bigg(1 - \frac{\mu}{L}\bigg)^t \| x_0 - x^* \|^2 \\
						= & \bigg(1 - \frac{1}{\kappa}\bigg)^t \| x_0 - x^* \|^2
\end{aligned}
$$
where $\kappa = L/\mu$.
\end{theorem}

\begin{theorem}[Gradient Descent convergence for convex quadratic functions] \label{thm:quad_gd_convergence}
Let $f: \Re^m \rightarrow \Re$ be a L-Lipschitz continuous and $\mu$-strongly convex quadratic function. Then the Gradient Descent with step size $\alpha = \displaystyle \frac{2}{L + \mu}$ and momentum $\beta = \displaystyle \frac{\kappa-1}{\kappa+1} = 1 - \frac{2}{\kappa+1}$ satisfies:
$$
\begin{aligned}
	\| x_t - x^* \| = \bigg(\frac{\kappa-1}{\kappa+1}\bigg)^t \| x_0 - x^* \|
\end{aligned}
$$
where $\kappa = L/\mu$.
\end{theorem}

The Gradient Descent \emph{convergence rate} for L-Lipschitz continuous convex functions is $\displaystyle \mathcal{O}\Bigg(\frac{1}{t}\Bigg)$ and $\displaystyle \mathcal{O}\Bigg(\exp\Bigg(-\frac{t}{\kappa}\Bigg)\Bigg)$ for L-Lipschitz continuous and strongly convex functions. We can also write the \emph{iteration complexity}, i.e., the smallest $t$ such that we’re within $\epsilon$-close to global optimum, as $\displaystyle \mathcal{O}\Bigg(\frac{1}{\epsilon}\Bigg)$ for L-Lipschitz continuous convex functions and as $\displaystyle \mathcal{O}\Bigg(\kappa \log \frac{1}{\epsilon}\Bigg)$ for L-Lipschitz continuous and strongly convex functions.

% http://www.princeton.edu/~yc5/ele522_optimization/lectures/accelerated_gradient.pdf

\subsubsection{Momentum}

To mitigate the pathological zig-zagging by speeding up the \emph{convergence rate} of the SGD method, we introduce two accelerated methods~\cite{polyak1964some} and~\cite{nesterov1998introductory, nesterov1983method} that exploits information from the history, i.e., past iterates, to add some inertia, i.e., the momentum, to yield smoother trajectory.

In the Polyak's method~\cite{polyak1964some} the velocity vector $v_t$ is calculated by applying the $\beta$ momentum to the previous $v_{t-1}$ displacement, and subtracting the gradient step to $x_t$.

% http://mitliagkas.github.io/ift6085-2019/ift-6085-lecture-6-notes.pdf

\begin{figure}[h!]
	\centering
  	\includegraphics[scale=0.5]{img/momentum}
  	\caption{Polyak's and Nesterov's Momentum}
  	\label{fig:momentum}
\end{figure}

\begin{algorithm}[H]
	\caption{Polyak's Accelerated Gradient Descent or Polyak Heavy-Ball method}
	\label{alg:pag}
	\begin{algorithmic}
		\Require{Function $f$ to minimize}
		\Require{Learning rate or step size $\alpha > 0$}
		\Require{Momentum $\beta \in [0,1)$}
		\Function{PolyakAcceleratedGradientDescent}{$f,\alpha,\beta$}
			\State Initialize weight vector $x_1 \gets x_0$ and velocity vector $v_0 \gets 0$
			\State $t \gets 1$
			\While{$not\_convergence$}
				\State $v_t = \beta v_{t-1} + \alpha \nabla f(x_t)$
				\State $x_{t+1} = x_t - v_t$
				\State $t \gets t + 1$
			\EndWhile
			\State \Return $x_t$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

% https://angms.science/doc/CVX/CVX_HBM.pdf

\begin{theorem}[Polyak's Accelerated Gradient Descent convergence for convex quadratic functions] \label{thm:quad_pag_convergence}
Let $f: \Re^m \rightarrow \Re$ be a L-Lipschitz continuous and $\mu$-strongly convex quadratic function. Then the Polyak's Accelerated Gradient Descent with step size $\alpha = \displaystyle \frac{4}{(\sqrt{L} + \sqrt{\mu})^2}$ and momentum $\beta = \displaystyle \frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1} = 1 - \frac{2}{\sqrt{\kappa}+1}$ satisfies:
$$
\begin{aligned}
	\| x_t - x^* \| = \bigg(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\bigg)^t \| x_0 - x^* \|
\end{aligned}
$$
where $\kappa = L/\mu$.
\end{theorem}

Leveraging the idea of momentum introduced by Polyak, Nesterov introduced a slightly altered update rule that has been shown to converge not only for quadratic functions, but for general convex functions. In the Nesterov's method~\cite{nesterov1998introductory}, instead, the velocity vector $v_t$ is calculated by applying the $\beta$ momentum to the previous $v_{t-1}$ displacement, and subtracting the gradient step to $x_t + \beta v_{t-1}$, which is the point where the momentum term leads from $x_t$.

\begin{algorithm}[H]
	\caption{Nesterov's Accelerated Gradient Descent or Nesterov Heavy-Ball method}
	\label{alg:nag}
	\begin{algorithmic}
		\Require{Function $f$ to minimize}
		\Require{Learning rate $\alpha > 0$}
		\Require{Momentum $\beta \in [0,1)$}
		\Function{NesterovAcceleratedGradientDescent}{$f,\alpha,\beta$}
			\State Initialize weight vector $x_1 \gets x_0$ and velocity vector $v_0 \gets 0$
			\State $t \gets 1$
			\While{$not\_convergence$}
				\State $\hat{x}_t \gets x_t + \beta v_{t-1}$
				\State $v_t \gets \beta v_{t-1} + \alpha \nabla f(\hat{x}_t)$
				\State $x_{t+1} \gets x_t - v_t$
				\State $t \gets t + 1$
			\EndWhile
			\State \Return $x_t$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Comparing the algorithm~\ref{alg:pag} with the algorithm~\ref{alg:nag}, we can see that Polyak’s method evaluates the gradient before adding momentum, whereas Nesterov’s algorithm evaluates it after applying momentum, which intuitively brings us closer to the minimum $x^*$, as showb in figure~\ref{fig:momentum}.

\begin{theorem}[Nesterov's Accelerated Gradient Descent convergence for convex functions] \label{thm:cvx_nag_convergence}
Let $f: \Re^m \rightarrow \Re$ be a L-Lipschitz continuous convex function. Then the Nesterov's Accelerated Gradient Descent with step size $\alpha \leq 1/L$ and momentum $\beta_{t+1} = t / (t+3)$ satisfies:
$$
f(x_t) - f(x^*) \leq \frac{2 \| x_0 - x^* \|^2}{\alpha (t+1)^2}
$$
In particular, for $\alpha = 1/L$:
$$
f(x_t) - f(x^*) \leq \frac{2L \| x_0 - x^* \|^2}{(t+1)^2}
$$
\end{theorem}

\begin{theorem}[Nesterov's Accelerated Gradient Descent convergence for strongly convex functions] \label{thm:str_cvx_nag_convergence}
Let $f: \Re^m \rightarrow \Re$ be a L-Lipschitz continuous and $\mu$-strongly convex function. Then the Nesterov's Accelerated Gradient Descent with step size $\alpha \leq 1/L$ and momentum $\beta_t = \displaystyle \frac{1 - \sqrt{\mu / L}}{1 + \sqrt{\mu / L}} = \frac{1-1/\sqrt{\kappa}}{1+1/\sqrt{\kappa}}$ satisfies:
$$
\begin{aligned}
	f(x_t) - f(x^*) \leq & \frac{\| x_0 - x^* \|^2}{\alpha} \Bigg(1 - \sqrt{\frac{\mu}{L}}\Bigg)^t \\
						= & \frac{\| x_0 - x^* \|^2}{\alpha} \Bigg(1 - \frac{1}{\sqrt{\kappa}}\Bigg)^t
\end{aligned}
$$
In particular, for $\alpha = 1/L$:
$$
\begin{aligned}
	f(x_t) - f(x^*) \leq & L \| x_0 - x^* \|^2 \Bigg(1 - \sqrt{\frac{\mu}{L}}\Bigg)^t \\
						= & L \| x_0 - x^* \|^2 \Bigg(1 - \frac{1}{\sqrt{\kappa}}\Bigg)^t
\end{aligned}
$$
where $\kappa = L/\mu$.
\end{theorem}

% https://blogs.princeton.edu/imabandit/2014/03/06/nesterovs-accelerated-gradient-descent-for-smooth-and-strongly-convex-optimization/

Nesterov's momentum brings the \emph{convergence rate} from $\displaystyle \mathcal{O}\Bigg(\frac{1}{t}\Bigg)$ to $\displaystyle \mathcal{O}\Bigg(\frac{1}{t^2}\Bigg)$ and in the case of strongly convex functions gives the acceleration that we had with Polyak’s momentum for quadratic functions, i.e., $\displaystyle \mathcal{O}\Bigg(\exp\Bigg(-\frac{t}{\sqrt{\kappa}}\Bigg)\Bigg)$. This is great because we get the guarantee for a more general class of functions but this rate of convergence cannot be further improved only using first-order information. We can also write the \emph{iteration complexity}, i.e., the smallest $t$ such that we’re within $\epsilon$-close to global optimum, for a L-Lipschitz continuous and $\mu$-strongly convex function as $\displaystyle \mathcal{O}\Bigg(\sqrt{\kappa}\log \frac{1}{\epsilon}\Bigg)$ for the accelerated methods where $\kappa$, i.e., the \emph{conditioning number}, is defined as $\kappa = L/\mu$ and where $L$ and $\mu$ are also equal to the largest $\lambda_{max}$ and the smallest $\lambda_{min}$ eigenvalues respectively. Finally, NAG's \emph{iteration complexity} for L-Lipschitz continuous convex functions is $\displaystyle \mathcal{O}\Bigg(\frac{1}{\sqrt{\epsilon}}\Bigg)$.

\pagebreak

\subsection{Sequential Minimal Optimization for Wolfe Dual formulations}

The \emph{Sequential Minimal Optimization (SMO)}~\cite{platt1998sequential} method is the most popular approach for solving the SVM QP problem without any extra $Q$ matrix storage required by common QP methods. The advantage of SMO lies in the fact that it performs a series of two-point optimizations since we deal with just one equality constraint, so the Lagrange multipliers can be solved analitically.

\subsubsection{Classification}

At each iteration, SMO chooses two $\alpha_i$ to jointly optimize, let $\alpha_1$ and $\alpha_2$, finds the optimal values for these multipliers and update the SVM to reflect these new values. In order to solve for two Lagrange multipliers, SMO first computes the constraints over these and then solves for the constrained minimum. Since there are only two multipliers, the box-constraints cause the Lagrange multipliers to lie within a box, while the linear equality constraint causes the Lagrange multipliers to lie on a diagonal line inside the box. So, the constrained minimum must lie there as shown in~\ref{fig:smo_lagrange_multipliers}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{img/smo_multipliers}
	\caption{SMO for two Lagrange multipliers}
	\label{fig:smo_lagrange_multipliers}
\end{figure}

In case of classification the ends of the diagonal line segment, i.e., the lower and upper bounds, can be espressed as follow if the target $y_1 \ne y_2$:

\begin{equation} \label{eq:smo_svc_bounds_update1}
	\begin{aligned}
		& L = max(0, \alpha_2 - \alpha_1) \\
		& H = min(C, C + \alpha_2 - \alpha_1)
	\end{aligned}
\end{equation}

or, alternatively, if the target $y_1 = y_2$:

\begin{equation} \label{eq:smo_svc_bounds_update2}
	\begin{aligned}
		& L = max(0, \alpha_2 + \alpha_1 - C) \\
		& H = min(C, \alpha_2 + \alpha_1)
	\end{aligned}
\end{equation}

The second derivative of the objective quadratic function along the diagonl line can be expressed as:

\begin{equation} \label{eq:smo_eta}
	\eta = K(x_1, x_1) + K(x_2, x_2) - 2K(x_1, x_2)
\end{equation}

that will be grather than zero if the kernel matrix will be positive definite, so there will be a minimum along the linear equality constraints that will be:

\begin{equation} \label{eq:smo_svc_a2_new}
	\alpha_2^{new} = \alpha_2 + \frac{y_2(E_1 - E_2)}{\eta}
\end{equation}

where $E_i = y_i - y'_i$ is the error on the $i$-th training example and $y'_i$ is the output of the SVC for the same.

Then, the box-constrained minimum is found by clipping the unconstrained minimum to the ends of the line segment:

\begin{equation} \label{eq:smo_svc_a2_new_clipped}
    \alpha_2^{new,clipped} =
        \begin{cases}
            H & \text{if} \ \alpha_2^{new} \geq H \\
            \alpha_2^{new} & \text{if} \ L < \alpha_2^{new} < H \\
            L & \text{if} \ \alpha_2^{new} \leq L \\
        \end{cases}
\end{equation}

Finally, the value of $\alpha_1$ is computed from the new clipped $\alpha_2$ as:

\begin{equation} \label{eq:smo_svc_a1_new}
	\alpha_1^{new} = \alpha_1 + s (\alpha_2 - \alpha_2^{new,clipped})
\end{equation}

where $s = y_1 y_2$.

Since the \emph{Karush-Kuhn-Tucker} conditions are necessary and sufficient conditions for optimality of a positive definite QP problem and the KKT conditions for the classification problem~\eqref{eq:svc_min_wolfe_dual} are:

\begin{equation} \label{eq:svc_smo_kkt}
	\begin{aligned}
		\alpha_i = 0 & \Leftrightarrow y_i y'_i \geq 1 \\
		0 < \alpha_i < C & \Leftrightarrow y_i y'_i = 1 \\
		\alpha_i = C & \Leftrightarrow y_i y'_i \leq 1
	\end{aligned}
\end{equation}

the steps described above will be iterate as long as there will be an example that violates them.

After optimizing $\alpha_1$ and $\alpha_2$, we select the threshold $b$ such that the KKT conditions are satisfied for $x_1$ and $x_2$. If, after optimization, $\alpha_1$ is not at the bounds, i.e., $0 < \alpha_1 < C$, then the following threshold $b_{up}$ is valid, since it forces the SVC to output $y_1$ when the input is $x_1$:

\begin{equation} \label{eq:smo_svc_b1}
	b_{up} = E_1 + y_1 (\alpha_1^{new} - \alpha_1) K(x_1,x_1) + y_2 (\alpha_2^{new,clipped} - \alpha_2) K(x_1,x_2) + b
\end{equation}

similarly, the following threshold $b_{low}$ is valid if $0 < \alpha_2 < C$:

\begin{equation} \label{eq:smo_svc_b2}
	b_{low} = E_2 + y_1 (\alpha_1^{new} - \alpha_1) K(x_1,x_2) + y_2 (\alpha_2^{new,clipped} - \alpha_2) K(x_2,x_2) + b
\end{equation}

If, after optimization, both $0 < \alpha_1 < C$ and $0 < \alpha_2 < C$ then both these thresholds are valid, and they will be equal; else, if both $\alpha_1$ and $\alpha_2$ are at the bounds, i.e., $\alpha_1 = 0$ or $\alpha_1 = C$ and $\alpha_2 = 0$ or $\alpha_2 = C$, then all the thresholds between $b_{up}$ and $b_{low}$ satisfy the KKT conditions, so we choose the threshold to be halfway in between $b_{up}$ and $b_{low}$. This gives the complete equation for $b$:

\begin{equation} \label{eq:smo_svc_b}
	b =
        \begin{cases}
            b_{up} & \text{if} \ 0 < \alpha_1 < C \\
            b_{low} & \text{if} \ 0 < \alpha_2 < C \\
            \displaystyle \frac{b_{up}+b_{low}}{2} & \text{otherwise} \\
        \end{cases}
\end{equation}

\newpage

\begin{breakablealgorithm}
	\caption{Sequential Minimal Optimization for Classification}
	\label{alg:smo_classifier}
	\begin{algorithmic}
		\Require{Training examples matrix $X \in \Re^{n \times m}$}
		\Require{Training target vector $y \in \pm1^n$}
		\Require{Kernel matrix $K \in \Re^{n \times n}$}
		\Require{Regularization parameter $C > 0$}
		\Require{Tolerance value $tol$ for stopping criterion}
		\Function{SMOClassifier}{$X,y,K,C,tol$}
			\State Initialize the Lagrange multipliers vector $\alpha \in \Re^n, \alpha \gets 0$
			\State Initialize the empty set $I0 \gets \{i : 0 < \alpha_i < C\}$
			\State Initialize the set $I1 \gets \{i : y_i = +1, \alpha_i = 0\}$ to contain all the indices of the training examples of class $+1$
			\State Initialize the empty set $I2 \gets \{i : y_i = -1,	 \alpha_i = C\}$
			\State Initialize the empty set $I3 \gets \{i :  y_i = +1,	 \alpha_i = C\}$
			\State Initialize the set $I4 \gets \{i : y_i = -1, \alpha_i = 0\}$ to contain all the indices of the training examples of class $-1$
			\State Initialize $b_{up} \gets -1$
			\State Initialize $b_{low} \gets +1$
			\State Initialize the error cache vector $errors \in \Re^n, errors \gets 0$
			\While {$num\_changed > 0$ \OR $examine\_all = True$}
				\State $num\_changed \gets 0$
				\State $examine\_all \gets True$
				\If {$examine\_all = True$}
					\For {$i \gets 0$ to $n$} \Comment loop over all training examples
						\State $num\_changed \gets num\_changed + \Call{ExamineExample}{i}$
					\EndFor
				\Else
					\For {$i$ in $I0$} \Comment loop over examples where $\alpha_i$ are not already at their bounds
						\State $num\_changed \gets num\_changed + \Call{ExamineExample}{i}$
						\If {$b_{up} > b_{low} - 2 tol$} \Comment check if optimality on $I0$ is attained
							\State {$num\_changed \gets 0$}
							\Break
						\EndIf
					\EndFor
				\EndIf
				\If {$examine\_all = True$}
					\State $examine\_all \gets False$
				\ElsIf {$num\_changed = 0$}
					\State $examine\_all \gets True$
				\EndIf
			\EndWhile
			\State Compute $b$ by~\eqref{eq:smo_svc_b}
			\State \Return $\alpha,b$
		\EndFunction
	\end{algorithmic}
	
	\newpage
	
	\begin{algorithmic}
		\Require{$i2$-th Lagrange multiplier}
		\Function{ExamineExample}{$i2$}
			\If {$i2$ in $I0$}
				\State $E_2 \gets errors_{i2}$
			\Else 
				\State Compute $E_2$
				\State $errors_{i2} \gets E_2$
				\State Update $(b_{low}, i_{low})$ or $(b_{up}, i_{up})$ using $(E_2, i2)$
			\EndIf
			\If {optimality is attained using current $b_{low}$ and $b_{up}$}
				\State \Return 0
			\Else
				\State Find an index $i1$ to do joint optimization with $i2$
				\If {$\Call{TakeStep}{i1,i2}$ = True}
					\State \Return 1
				\Else
					\State \Return 0
				\EndIf
			\EndIf
		\EndFunction
	\end{algorithmic}
	
	\newpage
	
	\begin{algorithmic}
		\Require{$i1$-th Lagrange multiplier}
		\Require{$i2$-th Lagrange multiplier}
		\Function{TakeStep}{$i1,i2$}
			\If {$i1 = i2$}
				\State \Return False
			\EndIf
			\State Compute $L$ and $H$ using~\eqref{eq:smo_svc_bounds_update1} or~\eqref{eq:smo_svc_bounds_update2}
			\If {$L = H$}
				\State \Return False
			\EndIf
			\State Compute $\eta$ by~\eqref{eq:smo_eta} \Comment we assume that $\eta > 0$, i.e., the kernel matrix $K$ is positive definite
			\If {$\eta < 0$}
				\State Choose $\alpha_2^{new,clipped}$ between $L$ and $H$ according to the largest value of the objective function at these points
			\Else
				\State Compute $\alpha_2^{new}$ by~\eqref{eq:smo_svc_a2_new}
				\State Compute $\alpha_2^{new,clipped}$ by~\eqref{eq:smo_svc_a2_new_clipped}
			\EndIf
			\If {changes in $\alpha_2^{new,clipped}$ are larger than some eps}
				\State Compute $\alpha_1^{new}$ by~\eqref{eq:smo_svc_a1_new}
				\State Update $\alpha_2^{new,clipped}$ and $\alpha_1^{new}$
				\For {$i$ in $I0$}
					\State Update $errors_i$ using new Lagrange multipliers
				\EndFor
				\State Update $\alpha$ using new Lagrange multipliers
				\State Update $I0, I1, I2, I3$ and $I4$
				\State Update $errors_{i1}$ and $errors_{i2}$
				\For {$i$ in $I0 \cup \{i1,i2\}$}
					\State Compute $(i_{low}, b_{low})$ by $b_{low} = \max\{errors_i : i \in I0 \cup I3 \cup I4\}$
					\State Compute $(i_{up}, b_{up})$ by $b_{up} = \min\{errors_i : i \in I0 \cup I1 \cup I2\}$
				\EndFor
				\State \Return True
			\Else
				\State \Return False
			\EndIf
		\EndFunction
	\end{algorithmic}
\end{breakablealgorithm}

\newpage

\subsubsection{Regression}

In case of regression the bounds and the new multipliers $\alpha_1^{+,new}$ and $\alpha_2^{+,new}$ can be expressed as follows if ($\alpha_1^+ > 0$ or ($\alpha_1^- = 0$ and $ E_1 - E_2 > 0$)) and ($\alpha_2^+ > 0$ or ($\alpha_2^- = 0$ and $ E_1 - E_2 < 0$)):

\begin{equation} \label{eq:smo_svr_bounds_update1}
	\begin{aligned}
		& L = max(0, \gamma - C) \\
		& H = min(C, \gamma)
	\end{aligned}
\end{equation}

\begin{equation} \label{eq:smo_svr_a2_new1}
	\alpha_2^{+,new} = \alpha_2^+ - \frac{E_1 - E_2}{\eta}
\end{equation}

\begin{equation} \label{eq:smo_svr_a1_new1}
	\alpha_1^{+,new} = \alpha_1^+ - (\alpha_2^{+,new,clipped} - \alpha_2^+)
\end{equation}

or, if ($\alpha_1^+ > 0$ or ($\alpha_1^- = 0$ and $ E_1 - E_2 > 2 \epsilon$)) and ($\alpha_2^- > 0$ or ($\alpha_2^+ = 0$ and $ E_1 - E_2 > 2 \epsilon$)):

\begin{equation} \label{eq:smo_svr_bounds_update2}
	\begin{aligned}
		& L = max(0, -\gamma) \\
		& H = min(C, -\gamma + C)
	\end{aligned}
\end{equation}

\begin{equation} \label{eq:smo_svr_a2_new2}
	\alpha_2^{-,new} = \alpha_2^- + \frac{(E_1 - E_2) - 2 \epsilon}{\eta}
\end{equation}

\begin{equation} \label{eq:smo_svr_a1_new2}
	\alpha_1^{+,new} = \alpha_1^+ + (\alpha_2^{-,new,clipped} - \alpha_2^-)
\end{equation}

or, if ($\alpha_1^- > 0$ or ($\alpha_1^+ = 0$ and $ E_1 - E_2 < - 2 \epsilon$)) and ($\alpha_2^+ > 0$ or ($\alpha_2^- = 0$ and $ E_1 - E_2 < - 2 \epsilon$)):

\begin{equation} \label{eq:smo_svr_bounds_update3}
	\begin{aligned}
		& L = max(0, \gamma) \\
		& H = min(C, C + \gamma)
	\end{aligned}
\end{equation}

\begin{equation} \label{eq:smo_svr_a2_new3}
	\alpha_2^{+,new} = \alpha_2^+ - \frac{(E_1 - E_2) + 2 \epsilon}{\eta}
\end{equation}

\begin{equation} \label{eq:smo_svr_a1_new3}
	\alpha_1^{-,new} = \alpha_1^- + (\alpha_2^{+,new,clipped} - \alpha_2^+)
\end{equation}

or, finally, if ($\alpha_1^- > 0$ or ($\alpha_1^+ = 0$ and $ E_1 - E_2 < 0$)) and ($\alpha_2^- > 0$ or ($\alpha_2^+ = 0$ and $ E_1 - E_2 > 0$)):

\begin{equation} \label{eq:smo_svr_bounds_update4}
	\begin{aligned}
		& L = max(0, -\gamma - C) \\
		& H = min(C, -\gamma)
	\end{aligned}
\end{equation}

\begin{equation} \label{eq:smo_svr_a2_new4}
	\alpha_2^{-,new} = \alpha_2^- + \frac{E_1 - E_2}{\eta}
\end{equation}

\begin{equation} \label{eq:smo_svr_a1_new4}
	\alpha_1^{-,new} = \alpha_1^- - (\alpha_2^{-,new,clipped} - \alpha_2^-)
\end{equation}

where $\gamma = \alpha_1^+ - \alpha_1^- + \alpha_2^+ - \alpha_2^-$. Notice that $\eta$ and $\alpha_2^{+,new,clipped}$ or $\alpha_2^{-,new,clipped}$ are identical to~\eqref{eq:smo_eta} and~\eqref{eq:smo_svc_a2_new_clipped} respectively.

The KKT conditions for the regression problem~\eqref{eq:svr_min_wolfe_dual} are:

\begin{equation} \label{eq:svr_smo_kkt}
	\begin{aligned}
		\alpha_i^+ - \alpha_i^- = 0 & \Leftrightarrow | y_i - y'_i | < \epsilon \\
		-C < \alpha_i^+ - \alpha_i^- < C & \Leftrightarrow | y_i - y'_i | = \epsilon \\
		\alpha_i^+ + \alpha_i^- = C & \Leftrightarrow | y_i - y'_i | > \epsilon	
	\end{aligned}
\end{equation}

so, the steps described above will be iterate as long as there will be an example that violates them.

In case of regression we select the threshold $b$ as follows:

\begin{equation} \label{eq:smo_svr_b1}
	b_{up} = E_1 + ((\alpha_1^+ - \alpha_1^-) - (\alpha_1^{+,new} - \alpha_1^{-,new})) K(x_1,x_1) + ((\alpha_2^+ - \alpha_2^-) - (\alpha_2^{+,new,clipped} - \alpha_2^{-,new,clipped})) K(x_1,x_2) + b
\end{equation}

\begin{equation} \label{eq:smo_svr_b2}
	b_{low} = E_2 + ((\alpha_1^+ - \alpha_1^-) - (\alpha_1^{+,new} - \alpha_1^{-,new})) K(x_1,x_2) + ((\alpha_2^+ - \alpha_2^-) - (\alpha_2^{+,new,clipped} - \alpha_2^{-,new,clipped})) K(x_2,x_2) + b
\end{equation}

\begin{equation} \label{eq:smo_svr_b}
	b =
        \begin{cases}
            b_{up} & \text{if} \ 0 < \alpha_1^+, \alpha_1^- < C \\
            b_{low} & \text{if} \ 0 < \alpha_2^+, \alpha_2^- < C \\
            \displaystyle \frac{b_{up}+b_{low}}{2} & \text{otherwise} \\
        \end{cases}
\end{equation}

\bigskip
\bigskip

The improvements described in~\cite{keerthi2001improvements, shevade1999improvements} for classification and regression respectively are about the definition of subsets of multipliers to efficiently update them at each iteration by separating the multipliers at the bounds from those who can be further minimized.

\newpage

\begin{breakablealgorithm}
	\caption{Sequential Minimal Optimization for Regression}
	\label{alg:smo_regression}
	\begin{algorithmic}
		\Require{Training examples matrix $X \in \Re^{n \times m}$}
		\Require{Training target vector $y \in \Re^n$}
		\Require{Kernel matrix $K \in \Re^{n \times n}$}
		\Require{Regularization parameter $C > 0$}
		\Require{Epsilon-tube value $\epsilon \geq 0$ within which no penalty is associated in the epsilon-insensitive loss function}
		\Require{Tolerance value $tol$ for stopping criterion}
		\Function{SMORegression}{$X,y,K,C,\epsilon,tol$}
			\State Initialize the Lagrange multipliers vector $\alpha^+ \in \Re^n, \alpha^+ \gets 0$
			\State Initialize the Lagrange multipliers vector $\alpha^- \in \Re^n, \alpha^- \gets 0$
			\State Initialize the empty set $I0 \gets \{i : 0 < \alpha^+_i, \alpha^-_i < C\}$
			\State Initialize the set $I1 \gets \{i : \alpha^+_i = 0, \alpha^-_i = 0\}$ to contain all the indices of the training examples
			\State Initialize the empty set $I2 \gets \{i : \alpha^+_i = 0, \alpha^-_i = C\}$
			\State Initialize the empty set $I3 \gets \{i : \alpha^+_i = C, \alpha^-_i = 0\}$
			\State Initialize $i_{up} \gets 0$ \Comment or any other target index $i_{up}$ from the training examples
			\State Initialize $i_{low} \gets 0$ \Comment or any other target index $i_{low}$ from the training examples
			\State Initialize $b_{up} \gets y_{i_{up}} + \epsilon$
			\State Initialize $b_{low} \gets y_{i_{low}} - \epsilon$
			\State Initialize the error cache vector $errors \in \Re^n, errors \gets 0$
			\While {$num\_changed > 0$ \OR $examine\_all = True$}
				\State $num\_changed \gets 0$
				\State $examine\_all \gets True$
				\If {$examine\_all = True$}
					\For {$i \gets 0$ to $n$} \Comment loop over all training examples
						\State $num\_changed \gets num\_changed + \Call{ExamineExample}{i}$
					\EndFor
				\Else
					\For {$i$ in $I0$} \Comment loop over examples where $\alpha^+_i$ and $\alpha^-_i$ are not already at their bounds
						\State $num\_changed \gets num\_changed + \Call{ExamineExample}{i}$
						\If {$b_{up} > b_{low} - 2 tol$} \Comment check if optimality on $I0$ is attained
							\State {$num\_changed \gets 0$}
							\Break
						\EndIf
					\EndFor
				\EndIf
				\If {$examine\_all = True$}
					\State $examine\_all \gets False$
				\ElsIf {$num\_changed = 0$}
					\State $examine\_all \gets True$
				\EndIf
			\EndWhile
			\State Compute $b$ by~\eqref{eq:smo_svr_b}
			\State \Return $\alpha^+,\alpha^-,b$
		\EndFunction
	\end{algorithmic}
	
	\newpage
	
	\begin{algorithmic}
		\Require{$i1$-th Lagrange multiplier}
		\Require{$i2$-th Lagrange multiplier}
		\Function{TakeStep}{$i1,i2$}
			\If {$i1 = i2$}
				\State \Return False
			\EndIf
			\State $finished = False$
			\While {\NOT $finished$}
				\State Compute $L$ and $H$ using~\eqref{eq:smo_svr_bounds_update1},~\eqref{eq:smo_svr_bounds_update2},~\eqref{eq:smo_svr_bounds_update3} or~\eqref{eq:smo_svr_bounds_update4}
				\If {$L < H$}
					\State Compute $\eta$ by~\eqref{eq:smo_eta} \Comment we assume that $\eta > 0$, i.e., the kernel matrix $K$ is positive definite
					\If {$\eta < 0$}
						\State Choose $\alpha_2^{+,new,clipped}$ or $\alpha_2^{-,new,clipped}$ between $L$ and $H$ according to the largest value of the objective function at these points
					\Else
						\State Compute $\alpha_2^{+,new}$ or $\alpha_2^{-,new}$ using~\eqref{eq:smo_svr_a2_new1},~\eqref{eq:smo_svr_a2_new3} or~\eqref{eq:smo_svr_a2_new2},~\eqref{eq:smo_svr_a2_new4} respectively
						\State Compute $\alpha_2^{+,new,clipped}$ or $\alpha_2^{-,new,clipped}$ by~\eqref{eq:smo_svc_a2_new_clipped}
					\EndIf
					\State Compute $\alpha_1^{+,new}$ or $\alpha_1^{-,new}$ using~\eqref{eq:smo_svr_a1_new1},~\eqref{eq:smo_svr_a1_new2} or~\eqref{eq:smo_svr_a1_new3},~\eqref{eq:smo_svr_a1_new4} respectively
					\If {changes in $\alpha_2^{+,new,clipped}, \alpha_2^{-,new,clipped}, \alpha_1^{+,new}$ or $\alpha_1^{-,new}$ are larger than some eps}
						\State Update $\alpha_2^{+,new,clipped}, \alpha_2^{-,new,clipped}, \alpha_1^{+,new}$ or $\alpha_1^{-,new}$
					\EndIf
				\Else
					\State $finished = True$
				\EndIf
			\EndWhile
			\If {changes in $\alpha_2^{+,new,clipped}, \alpha_2^{-,new,clipped}, \alpha_1^{+,new}$ or $\alpha_1^{-,new}$ are larger than some eps}
				\For {$i$ in $I0$}
					\State Update $errors_i$ using new Lagrange multipliers
				\EndFor
				\State Update $\alpha^+$ and $\alpha^-$ using new Lagrange multipliers
				\State Update $I0, I1, I2$ and $I3$
				\State Update $errors_{i1}$ and $errors_{i2}$
				\For {$i$ in $I0 \cup \{i1,i2\}$}
					\State Compute $(i_{low}, b_{low})$ by $b_{low} = \max\{errors_i : i \in I0 \cup I1 \cup I2\}$
					\State Compute and $(i_{up}, b_{up})$ by $b_{up} = \min\{errors_i : i \in I0 \cup I1 \cup I3\}$
				\EndFor
				\State \Return True
			\Else
				\State \Return False
			\EndIf
		\EndFunction
	\end{algorithmic}
\end{breakablealgorithm}

\pagebreak

\subsection{AdaGrad for Lagrangian Dual formulations}

Due to the sparsity of the weight vector of the \emph{Lagrangian dual}, i.e., the Lagrange multipliers, we might end up in a situation where some components of the gradient are very small and others large. This, in terms of \emph{conditioning number}, i.e., $\kappa = L/\mu \gg 1$, means that the level sets of $f$ are ellipsoid, i.e., we are dealing with an ill-conditioned problem. So, given a learning rate, a standard gradient descent approach might end up in a situation where it decreases too quickly the small weights or too slowly the large ones.

Another method, that is usually deprecated in ML applications due to its increased computational complexity, is Newton’s method. Newton’s method favors a much faster \emph{convergence rate}, i.e., number of iterations, at the cost of being more expensive per iteration. For convex problems, the recursion is similar to the gradient descent algorithm:

$$
x_{t+1} = x_t - \alpha H^{-1} \nabla f(x_t)
$$

where $\alpha$ is often close to one (damped-Newton) or one, and $H^{-1}$ denotes the Hessian of $f$ at the current point, i.e., $\nabla^2 f(x_t)$.

The above suggest a general rule in optimization: find any preconditioner, in convex optimization it has to be positive semidefinite, that improves the performance of gradient descent in terms of iterations, but without wasting too much time to compute that precoditioner. The above result into:

$$
x_{t+1} = x_t - \alpha P^{-1} \nabla f(x_t)
$$

where $P$ is the preconditioner. This idea is the basis of the BFGS quasi-Newton method.

The \emph{AdaGrad}~\cite{duchi2011adaptive} algorithm is just a variant of preconditioned gradient descent, where $P$ is selected to be a diagonal preconditioner matrix and is updated using the gradient information, in particular it is the diagonal approximation of the inverse of the square roots of gradient outer products, until the $k$-th iteration. The above lead to the algorithm:

\begin{algorithm}[H]
	\caption{AdaGrad}
	\label{alg:adagrad}
	\begin{algorithmic}
		\Require{Function $f$ to minimize}
		\Require{Learning rate or step size $\alpha > 0$}
		\Require{Offset $\epsilon > 0$ to ensures not divide by 0}
		\Function{AdaGrad}{$f,\alpha,\epsilon$}
			\State Initialize weight vector $x_0$ and the squared accumulated gradients vector $s_t \gets 0$
			\State $t = 1$
			\While {$not\_convergence$}
				\State $g_t \gets \partial f(x_t)$ \Comment if $f$ is differentiable then $\partial f(x_t) = \nabla f(x_t)$
				\State $s_t \gets s_{t-1} + g_t^2$
				\State $x_{t+1} \gets x_t - \alpha P_t^{-1} g_t = x_t - \displaystyle \frac{\alpha}{\sqrt{s_t + \epsilon}} \odot g_t \ \text{where} \ P_t \gets diag(s_t + \epsilon)^{1/2}$
				\State $t \gets t + 1$
			\EndWhile
			\State \Return $x_t$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

In practical terms, \emph{AdaGrad} addresses the problem of the sparse optimal by adaptively scaling the learning rate for each dimension with the magnitude of the gradients. Coordinates that routinely correspond to large gradients are scaled down significantly, whereas others with small gradients receive a much more gentle treatment. \emph{AdaGrad}'s \emph{convergence rate} for L-Lipschitz continuous convex functions is the same of the SGD method described above.

\pagebreak

\subsection{Losses properties}

Several losses and objectives have been presented in section~\ref{section:svc} and~\ref{section:svr}. In our experiments, we will consider four different convex loss functions, two for the \emph{classification} and two for the \emph{regression} tasks. In particular, for what about the \emph{margin-based} losses, i.e., the \emph{classification} losses, both the \emph{hinge} and the \emph{squared hinge} losses are Lipschitz continuous and convex; meanwhile, for twhat about the \emph{distance-based} losses, i.e., the \emph{regression} losses, the \emph{epsilon-insensitive} loss is Lipschitz continuous and convex but the \emph{squared epsilon-insensitive} is convex and for this reason is locally Lipschitz continuous.

In general, if the objective function of a quadratic programming problem is strictly convex, i.e., the associated Hessian matrix is positive definite, the solution is unique. And if the objective function is convex, there may be cases where the solution is nonunique.

Assume that the hard margin SVM has a solution, i.e., the given problem is separable in the feature space. Then, since the objective function of the primal problem is $\displaystyle \frac{1}{2} \| w \|^2$, which is 1-strongly convex, the primal problem has a unique solution for $w$ and $b$.

Since the $\mathcal{L}_1$-SVM linearly penalizes the misclassified points, the primal objective function is convex. Likewise, the Hessian matrix of the dual objective function is positive semidefinite. Thus the primal and dual solutions may be nonunique. Meanwhile, the objective function of the primal problem for the $\mathcal{L}_2$-SVM is strictly convex, due to the quadratic penalization of the misclassified points. Therefore, $w$ and $b$ are uniquely determined if we solve the primal or dual problem. In summary, the following properties for the SVM's objectives are given:

% http://www.lib.kobe-u.ac.jp/repository/90000231.pdf

\begin{table}[H]
\centering
\caption{SVM's objectives properties for primal formulations}
\label{primal_svm_objectives_props}
\begin{tabular}{lrrr}
\toprule
	& smooth & \vtop{\hbox{\strut L-Lipschitz}\hbox{\strut continuous}} & convexity \\
objective & 		& 		& 		\\
\midrule
$\mathcal{L}_1$-SVC~\eqref{eq:biased_primal_svc_hinge} & no & globally & convex \\
$\mathcal{L}_2$-SVC~\eqref{eq:biased_primal_svc_squared_hinge} & yes & globally & \vtop{\hbox{\strut 1-strongly}\hbox{\strut convex}} \\
\midrule
$\mathcal{L}_1$-SVR~\eqref{eq:biased_primal_svr_eps} & no & globally & convex \\
$\mathcal{L}_2$-SVR~\eqref{eq:biased_primal_svr_squared_eps} & yes & locally & \vtop{\hbox{\strut 1-strongly}\hbox{\strut convex}} \\
\bottomrule
\end{tabular}
\end{table}

And, according to the theoretical analysis, the following \emph{convergence rates} are given for the primal and Lagrangian dual formulations respectively: 

% https://homepage.cs.uiowa.edu/~tyng/acml15-tutorial.pdf

\begin{table}[H]
\centering
\caption{SVM's objectives convergence rates for primal formulations}
\label{primal_svm_objectives_rates}
\begin{tabular}{lrrr}
\toprule
	& \vtop{\hbox{\strut SGD}\hbox{\strut convergence rate}} & \vtop{\hbox{\strut Polyak SGD}\hbox{\strut convergence rate}} & \vtop{\hbox{\strut Nesterov SGD}\hbox{\strut convergence rate}} \\
objective & 		& 		& 		\\
\midrule
$\mathcal{L}_1$-SVM~(\ref{eq:biased_primal_svc_hinge},~\ref{eq:biased_primal_svr_eps}) & $\displaystyle \mathcal{O}\Bigg(\frac{1}{\sqrt{t}}\Bigg)$ & $\displaystyle \mathcal{O}\Bigg(\frac{1}{\sqrt{t}}\Bigg)$ & $\displaystyle \mathcal{O}\Bigg(\frac{1}{\sqrt{t}}\Bigg)$ \\
$\mathcal{L}_2$-SVM~(\ref{eq:biased_primal_svc_squared_hinge},~\ref{eq:biased_primal_svr_squared_eps}) & $\displaystyle \mathcal{O}\Bigg(\frac{1}{t}\Bigg)$ & $\displaystyle \mathcal{O}\Bigg(\frac{1}{t}\Bigg)$ & $\displaystyle \mathcal{O}\Bigg(\exp\Bigg(-\frac{t}{\sqrt{\kappa}}\Bigg)\Bigg)$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{SVM's objectives convergence rate for Lagrangian dual formulations}
\label{dual_svm_objectives_props}
\begin{tabular}{lr}
\toprule
	& \vtop{\hbox{\strut AdaGrad}\hbox{\strut convergence rate}} \\
objective & 		\\
\midrule
$\mathcal{L}_1$-SVM~(\ref{eq:svc_lagrangian_dual},~\ref{eq:svr_lagrangian_dual}) or~(\ref{eq:svc_bcqp_lagrangian_dual},~\ref{eq:svr_bcqp_lagrangian_dual}) & $\displaystyle \mathcal{O}\Bigg(\frac{1}{\sqrt{t}}\Bigg)$ \\
\bottomrule
\end{tabular}
\end{table}

Notice that since the Hessian matrix $Q$ of the $\mathcal{L}_1$-SVM is not positive definite, i.e., the Lagrangian function is not strictly convex since it will be linear along the eigenvectors correspondent to the null eigenvalues and so it will be unbounded below, the Lagrangian dual relaxation, i.e.,~(\ref{eq:svc_lagrangian_sol},~\ref{eq:svc_bcqp_lagrangian_sol}) or~(\ref{eq:svr_lagrangian_sol},~\ref{eq:svr_bcqp_lagrangian_sol}), will be nondifferentiable, so it will have infinite solutions and for each of them it will have a different subgradient. In order to compute an approximation of the gradient, we will choose $\alpha$ in such a way as the one that minimizes the norm of the residual:

\begin{equation} \label{eq:lagrangian_krylov_sol}
	\min_{\alpha_n \in K_n(Q, b)} \| Q \alpha_n - b \|
\end{equation}

Since we are dealing with a symmetric but indefinite linear system we will choose a well-known Krylov method that performs the Lanczos iterate, i.e., symmetric Arnoldi iterate, called \emph{minres}, i.e., symmetric \emph{gmres}, to compute the vector $\alpha_n$ that minimizes the norm of the residual $r_n = Q \alpha_n - b$ among all vectors in $K_n(Q, b) = span(b, Qb, Q^2b, \dots, Q^{n-1}b)$.