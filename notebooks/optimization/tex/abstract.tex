\section{Abstract}

A \emph{Support Vector Machine} is a learning model used both for \emph{classification} and \emph{regression} tasks whose goal is to construct a \emph{maximum margin separator}, i.e., a decision boundary with the largest distance from the nearest training data points.

The aim of this report is to compare the \emph{primal}, the \emph{Wolfe dual}~\cite{fletcher2009support} and the \emph{Lagrangian dual} formulations of this model in terms of \emph{complexity}.

Firstly, a detailed mathematical derivation of the model for all these formulations is given, then three algorithms are described to solve the optimization problem in case of \emph{primal}, \emph{Wolfe dual} or \emph{Lagrangian dual} formulation of the problem, explaining their theoretical properties, i.e., \emph{convergence rate} and \emph{complexity}.

Finally, some experiments are shown for \emph{linearly} and \emph{nonlinearly} separable generated datasets to compare the performance with different \emph{hyperparameters} and different \emph{kernels}, also comparing the \emph{custom} results with \emph{liblinear}~\cite{fan2008liblinear} for the \emph{primal} formulations, \emph{libsvm}~\cite{chang2011libsvm} and \emph{cvxopt}~\cite{vandenberghe2010cvxopt} for the \emph{dual} ones.