\section{Track}

\begin{itemize}
\item[\texttt{(M1.1)}] is a \emph{Support Vector Classifier (SVC)} with the \emph{hinge} loss.

\begin{itemize}
\item[\texttt{(A1.1.1)}] is a \emph{momentum descent} approach~\cite{polyak1964some, nesterov1998introductory, nesterov1983method}, an \emph{accelerated gradient} method for solving the SVC in its \emph{primal} formulation.

\item[\texttt{(A1.1.2)}] is the \emph{Sequential Minimal Optimization (SMO)} algorithm~\cite{platt1998sequential, keerthi2001improvements}, an ad hoc \emph{active set} method for training a SVC in its \emph{Wolfe dual} formulation with \emph{linear}, \emph{polynomial} and \emph{gaussian} kernels.

\item[\texttt{(A1.1.3)}] is the \emph{AdaGrad} algorithm~\cite{duchi2011adaptive}, a \emph{deflected subgradient} method for solving the SVC in its \emph{Lagrangian dual} formulation with \emph{linear}, \emph{polynomial} and \emph{gaussian} kernels.
\end{itemize}

\end{itemize}

\begin{itemize}
\item[\texttt{(M1.2)}] is a \emph{Support Vector Classifier (SVC)} with the \emph{squared hinge} loss.

\begin{itemize}
\item[\texttt{(A1.2.1)}] is a \emph{momentum descent} approach~\cite{polyak1964some, nesterov1998introductory, nesterov1983method}, an \emph{accelerated gradient} method for solving the SVC in its \emph{primal} formulation.
\end{itemize}

\end{itemize}

\bigskip

\begin{itemize}
\item[\texttt{(M2.1)}] is a \emph{Support Vector Regression (SVR)} with the \emph{epsilon-insensitive} loss.

\begin{itemize}
\item[\texttt{(A2.1.1)}] is a \emph{momentum descent} approach~\cite{polyak1964some, nesterov1998introductory, nesterov1983method}, an \emph{accelerated gradient} method for solving the SVR in its \emph{primal} formulation.

\item[\texttt{(A2.1.2)}] is the \emph{Sequential Minimal Optimization (SMO)} algorithm~\cite{flake2002efficient, shevade1999improvements}, an ad hoc \emph{active set} method for training a SVR in its \emph{Wolfe dual} formulation with \emph{linear}, \emph{polynomial} and \emph{gaussian} kernels.

\item[\texttt{(A2.1.3)}] is the \emph{AdaGrad} algorithm~\cite{duchi2011adaptive}, a \emph{deflected subgradient} method for solving the SVR in its \emph{Lagrangian dual} formulation with \emph{linear}, \emph{polynomial} and \emph{gaussian} kernels.
\end{itemize}

\end{itemize}

\begin{itemize}
\item[\texttt{(M2.2)}] is a \emph{Support Vector Regression (SVR)} with the \emph{squared epsilon-insensitive} loss.

\begin{itemize}
\item[\texttt{(A2.2.1)}] is a \emph{momentum descent} approach~\cite{polyak1964some, nesterov1998introductory, nesterov1983method}, an \emph{accelerated gradient} method for solving the SVR in its \emph{primal} formulation.
\end{itemize}

\end{itemize}