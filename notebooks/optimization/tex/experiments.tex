\section{Experiments}

The following experiments refer to \emph{linearly} and \emph{nonlinearly} separable generated datasets of 100 training examples. All the training times refer to running on a laptop with an Intel i7-6700HQ (8) @ 3.500GHz and 31.2 GB of memory.

The Python source code is available at: \href{https://github.com/dmeoli/optiml}{\texttt{github.com/dmeoli/optiml}}.

\subsection{Support Vector Classifier}

Below experiments are about the SVC for which has been tested different values for the regularization hyperparameter $C$, i.e., from \emph{soft} to \emph{hard margin}, and in case of nonlinearly separable data also different \emph{kernel functions} mentioned above.

The experiments about SVCs are available at: \\ \href{https://github.com/dmeoli/optiml/blob/master/notebooks/optimization/CM_SVC_report_experiments.ipynb}{\texttt{github.com/dmeoli/optiml/blob/master/notebooks/optimization/CM\_SVC\_report\_experiments.ipynb}}.

\subsubsection{Hinge loss}

\paragraph{Primal formulation}

The experiments results shown in~\ref{primal_l1_svc_cv_results} referred to \emph{Stochastic Gradient Descent} algorithm are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.02 and $\beta$, i.e., the \emph{momentum}, equal to 0.5. The optimization process is stopped if after 5 iterations the function value does not improve by at least $1\mathrm{e}{-8}$.

\input{experiments/primal_l1_svc}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{img/l1_svc_loss_history}
	\caption{SGD convergence for the primal formulation of the $\protect \mathcal{L}_1$-SVC}
	\label{fig:l1_svc_loss_history}
\end{figure}

\paragraph{Linear dual formulations}

The experiments results shown in~\ref{linear_lagrangian_dual_l1_svc_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 1 for the \emph{AdaGrad} algorithm. Note that the \emph{unreg\_bias} and \emph{reg\_bias} duals refers to the \emph{Lagrangian dual} formulations~\eqref{eq:l1_svc_aug_lagrangian_dual} and~\eqref{eq:l1_svc_bcqp_aug_lagrangian_dual} respectively with $\rho$ equals to 1. The optimization process is stopped if the primal-dual weight vector does not change by at least $1\mathrm{e}{-6}$  between two consecutive iterations.

\input{experiments/linear_dual_l1_svc}

\input{experiments/linear_lagrangian_dual_l1_svc}

\paragraph{Nonlinear dual formulations}

The experiments results shown in~\ref{nonlinear_dual_l1_svc_cv_results} and~\ref{nonlinear_lagrangian_dual_l1_svc_cv_results} are obtained with \emph{d} and \emph{r} hyperparameters equal to 3 and 1 respectively for the \emph{polynomial} kernel; \emph{gamma} is setted to \emph{`scale`} for both \emph{polynomial} and \emph{gaussian} kernels. The experiments results shown in~\ref{nonlinear_lagrangian_dual_l1_svc_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 1 for the \emph{AdaGrad} algorithm. Note that the \emph{unreg\_bias} and \emph{reg\_bias} duals refers to the \emph{Lagrangian dual} formulations~\eqref{eq:l1_svc_aug_lagrangian_dual} and~\eqref{eq:l1_svc_bcqp_aug_lagrangian_dual} respectively with $\rho$ equals to 1. The optimization process is stopped if the primal-dual weight vector does not change by at least $1\mathrm{e}{-6}$  between two consecutive iterations.

\input{experiments/nonlinear_dual_l1_svc}

\input{experiments/nonlinear_lagrangian_dual_l1_svc}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{img/lagrangian_dual_l1_svc_loss_history}
	\caption{AdaGrad convergence for the Lagrangian dual formulation of the $\protect \mathcal{L}_1$-SVC}
	\label{fig:lagrangian_dual_l1_svc_loss_history}
\end{figure}

\pagebreak

\subsubsection{Squared hinge loss}

\paragraph{Primal formulation}

The experiments results shown in~\ref{primal_l2_svc_cv_results} referred to \emph{Stochastic Gradient Descent} algorithm are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.02 and $\beta$, i.e., the \emph{momentum}, equal to 0.5. The optimization process is stopped if after 5 iterations the function value does not improve by at least $1\mathrm{e}{-8}$.

\input{experiments/primal_l2_svc}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{img/l2_svc_loss_history}
	\caption{SGD convergence for the primal formulation of the $\protect \mathcal{L}_2$-SVC}
	\label{fig:l2_svc_loss_history}
\end{figure}

\pagebreak

\paragraph{Linear dual formulations}

The experiments results shown in~\ref{linear_lagrangian_dual_l2_svc_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 1 for the \emph{AdaGrad} algorithm. Note that the \emph{unreg\_bias} and \emph{reg\_bias} duals refers to the \emph{Lagrangian dual} formulations~\eqref{eq:l2_svc_aug_lagrangian_dual} and~\eqref{eq:l2_svc_lb_aug_lagrangian_dual} respectively with $\rho$ equals to 1. The optimization process is stopped if the primal-dual weight vector does not change by at least $1\mathrm{e}{-6}$  between two consecutive iterations.

\input{experiments/linear_lagrangian_dual_l2_svc}

\paragraph{Nonlinear dual formulations}

The experiments results shown in~\ref{nonlinear_lagrangian_dual_l2_svc_cv_results} are obtained with \emph{d} and \emph{r} hyperparameters equal to 3 and 1 respectively for the \emph{polynomial} kernel; \emph{gamma} is setted to \emph{`scale`} for both \emph{polynomial} and \emph{gaussian} kernels. The experiments results shown in~\ref{nonlinear_lagrangian_dual_l1_svc_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 1 for the \emph{AdaGrad} algorithm. Note that the \emph{unreg\_bias} and \emph{reg\_bias} duals refers to the \emph{Lagrangian dual} formulations~\eqref{eq:l2_svc_aug_lagrangian_dual} and~\eqref{eq:l2_svc_lb_aug_lagrangian_dual} respectively with $\rho$ equals to 1. The optimization process is stopped if the primal-dual weight vector does not change by at least $1\mathrm{e}{-6}$  between two consecutive iterations.

\input{experiments/nonlinear_lagrangian_dual_l2_svc}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{img/lagrangian_dual_l2_svc_loss_history}
	\caption{AdaGrad convergence for the Lagrangian dual formulation of the $\protect \mathcal{L}_2$-SVC}
	\label{fig:lagrangian_dual_l2_svc_loss_history}
\end{figure}

\pagebreak

\subsection{Support Vector Regression}

Below experiments are about the SVR for which has been tested different values for regularization hyperparameter $C$, i.e., from \emph{soft} to \emph{hard margin}, the $\epsilon$ penalty value and in case of nonlinearly separable data also different \emph{kernel functions} mentioned above.

The experiments about SVRs are available at: \\ \href{https://github.com/dmeoli/optiml/blob/master/notebooks/optimization/CM_SVR_report_experiments.ipynb}{\texttt{github.com/dmeoli/optiml/blob/master/notebooks/optimization/CM\_SVR\_report\_experiments.ipynb}}.

\subsubsection{Epsilon-insensitive loss}

\paragraph{Primal formulation}

The experiments results shown in~\ref{primal_l1_svr_cv_results} referred to \emph{Stochastic Gradient Descent} algorithm are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.02 and $\beta$, i.e., the \emph{momentum}, equal to 0.2. The optimization process is stopped if after 5 iterations the function value does not improve by at least $1\mathrm{e}{-8}$.

\input{experiments/primal_l1_svr}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{img/l1_svr_loss_history}
	\caption{SGD convergence for the primal formulation of the $\protect \mathcal{L}_1$-SVR}
	\label{fig:l1_svr_loss_history}
\end{figure}

\pagebreak

\paragraph{Linear dual formulations}

The experiments results shown in~\ref{linear_lagrangian_dual_l1_svr_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 1 for the \emph{AdaGrad} algorithm. Note that the \emph{unreg\_bias} and \emph{reg\_bias} duals refers to the \emph{Lagrangian dual} formulations~\eqref{eq:l1_svr_aug_lagrangian_dual} and~\eqref{eq:l1_svr_bcqp_aug_lagrangian_dual} respectively with $\rho$ equals to 1. The optimization process is stopped if the primal-dual weight vector does not change by at least $1\mathrm{e}{-6}$  between two consecutive iterations.

\input{experiments/linear_dual_l1_svr}

\input{experiments/linear_lagrangian_dual_l1_svr}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{img/linear_lagrangian_dual_l1_svr_loss_history}
	\caption{AdaGrad convergence for the Lagrangian dual formulation of the linear $\protect \mathcal{L}_1$-SVR}
	\label{fig:linear_lagrangian_dual_l1_svr_loss_history}
\end{figure}

\paragraph{Nonlinear dual formulations}

The experiments results shown in~\ref{nonlinear_dual_l1_svr_cv_results} and~\ref{nonlinear_lagrangian_dual_l1_svr_cv_results} are obtained with \emph{gamma} setted to \emph{`scale`} for both \emph{gaussian} and \emph{laplacian} kernels. The experiments results shown in~\ref{nonlinear_lagrangian_dual_l1_svc_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 1 for the \emph{AdaGrad} algorithm. Note that the \emph{unreg\_bias} and \emph{reg\_bias} duals refers to the \emph{Lagrangian dual} formulations~\eqref{eq:l1_svr_aug_lagrangian_dual} and~\eqref{eq:l1_svr_bcqp_aug_lagrangian_dual} respectively with $\rho$ equals to 1. The optimization process is stopped if the primal-dual weight vector does not change by at least $1\mathrm{e}{-6}$  between two consecutive iterations.

\input{experiments/nonlinear_dual_l1_svr}

\input{experiments/nonlinear_lagrangian_dual_l1_svr}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{img/laplacian_lagrangian_dual_l1_svr_loss_history}
	\caption{AdaGrad convergence for the Lagrangian dual formulation of the laplacian $\protect \mathcal{L}_1$-SVR}
	\label{fig:laplacian_lagrangian_dual_l1_svr_loss_history}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{img/gaussian_lagrangian_dual_l1_svr_loss_history}
	\caption{AdaGrad convergence for the Lagrangian dual formulation of the gaussian $\protect \mathcal{L}_1$-SVR}
	\label{fig:gaussian_lagrangian_dual_l1_svr_loss_history}
\end{figure}

\pagebreak

\subsubsection{Squared epsilon-insensitive loss}

\paragraph{Primal formulation}

The experiments results shown in~\ref{primal_l2_svr_cv_results} referred to \emph{Stochastic Gradient Descent} algorithm are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 0.02 and $\beta$, i.e., the \emph{momentum}, equal to 0.2. The optimization process is stopped if after 5 iterations the function value does not improve by at least $1\mathrm{e}{-8}$.

\input{experiments/primal_l2_svr}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5]{img/l2_svr_loss_history}
	\caption{SGD convergence for the primal formulation of the $\protect \mathcal{L}_2$-SVR}
	\label{fig:l2_svr_loss_history}
\end{figure}

\pagebreak

\paragraph{Linear dual formulations}

The experiments results shown in~\ref{linear_lagrangian_dual_l2_svr_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 1 for the \emph{AdaGrad} algorithm. Note that the \emph{unreg\_bias} and \emph{reg\_bias} duals refers to the \emph{Lagrangian dual} formulations~\eqref{eq:l2_svr_aug_lagrangian_dual} and~\eqref{eq:l2_svr_lb_aug_lagrangian_dual} respectively with $\rho$ equals to 1. The optimization process is stopped if the primal-dual weight vector does not change by at least $1\mathrm{e}{-6}$  between two consecutive iterations.

\input{experiments/linear_lagrangian_dual_l2_svr}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{img/linear_lagrangian_dual_l2_svr_loss_history}
	\caption{AdaGrad convergence for the Lagrangian dual formulation of the linear $\protect \mathcal{L}_2$-SVR}
	\label{fig:linear_lagrangian_dual_l2_svr_loss_history}
\end{figure}

\paragraph{Nonlinear dual formulations}

The experiments results shown in~\ref{nonlinear_lagrangian_dual_l2_svr_cv_results} are obtained with \emph{gamma} setted to \emph{`scale`} for both \emph{gaussian} and \emph{laplacian} kernels. The experiments results shown in~\ref{nonlinear_lagrangian_dual_l2_svr_cv_results} are obtained with $\alpha$, i.e., the \emph{learning rate} or \emph{step size}, setted to 1 for the \emph{AdaGrad} algorithm. Note that the \emph{unreg\_bias} and \emph{reg\_bias} duals refers to the \emph{Lagrangian dual} formulations~\eqref{eq:l2_svr_aug_lagrangian_dual} and~\eqref{eq:l2_svr_lb_aug_lagrangian_dual} respectively with $\rho$ equals to 1. The optimization process is stopped if the primal-dual weight vector does not change by at least $1\mathrm{e}{-6}$  between two consecutive iterations.

\input{experiments/nonlinear_lagrangian_dual_l2_svr}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{img/laplacian_lagrangian_dual_l2_svr_loss_history}
	\caption{AdaGrad convergence for the Lagrangian dual formulation of the laplacian $\protect \mathcal{L}_2$-SVR}
	\label{fig:laplacian_lagrangian_dual_l2_svr_loss_history}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.55]{img/gaussian_lagrangian_dual_l2_svr_loss_history}
	\caption{AdaGrad convergence for the Lagrangian dual formulation of the gaussian $\protect \mathcal{L}_2$-SVR}
	\label{fig:gaussian_lagrangian_dual_l2_svr_loss_history}
\end{figure}
