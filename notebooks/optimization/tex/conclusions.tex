\section{Conclusions}

For what about the SVM formulations, it is known, in general, that the \emph{primal formulation}, is suitable for large linear training since the complexity of the model grows with the number of features or, more in general, when the number of examples $n$ is much larger than the number of features $m$, i.e., $n \gg m$; meanwhile the \emph{dual formulation}, is more suitable in case the number of examples $n$ is less than the number of features $m$, i.e., $n < m$, since the complexity of the model is dominated by the number of examples, or more in general when the training data are not linearly separable in the input space.

\bigskip

From all these experiments we can see as all the \emph{custom} implementations underperforms all the others, i.e., both \emph{cvxopt}~\cite{vandenberghe2010cvxopt} and \emph{sklearn} implementations, i.e., \emph{liblinear}~\cite{fan2008liblinear} and \emph{libsvm}~\cite{chang2011libsvm} implementations, in terms of \emph{time} obviously due to the different core implementation languages, i.e., Python and C respectively.

In the \emph{primal} formulations the \emph{liblinear}~\cite{fan2008liblinear} implementation uses an optimization method called \emph{Coordinate Gradient Descent} which minimizes one coordinate at a time.

Meanwhile, for what about the \emph{Wolfe dual} formulations we can notice as \emph{cvxopt}~\cite{vandenberghe2010cvxopt} underperforms the \emph{sklearn} implementation, i.e., \emph{libsvm}~\cite{chang2011libsvm} implementation, in terms of \emph{time} since it is a general-purpose QP solver and it does not exploit the structure of the problem, as SMO does. An interesting consideration can be made about the number of \emph{iterations} of \emph{custom} SMO implementation wrt that in \emph{libsvm} which seems to be always lower thanks to the improvements described in~\cite{keerthi2001improvements, shevade1999improvements} for classification and regression respectively.

Finally, in the \emph{Lagrangian dual} formulations, we can see as fitting the intercept in an explicit way, i.e., by adding Lagrange multipliers to control the equality constraint always get lower scores wrt the \emph{Lagrangian dual} of the same problem with the bias term embedded into the weight matrix.

