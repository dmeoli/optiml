\section{Conclusions}

The actual \emph{convergence rates} of the \emph{primal} $\protect \mathcal{L}_1$-SVM formulations, i.e., the figures~\ref{fig:l1_svc_loss_history} and~\ref{fig:l1_svr_loss_history}, shows as they do not meet the theoretical expectations at the first line of the table~\ref{primal_svm_objectives_rates}. Both the \emph{Polyak} and the \emph{Nesterov} momentums provide a significant accelleration wrt the \emph{vanilla SGD} and they are quite comparable.

Conversely, the actual \emph{convergence rates} of the \emph{primal} $\protect \mathcal{L}_2$-SVC formulations, i.e., the figure~\ref{fig:l2_svc_loss_history} and~\ref{fig:l2_svr_loss_history}, shows as they do in part meet the theoretical expectations at the second line of the table~\ref{primal_svm_objectives_rates}. Despite the \emph{Nesterov} momentum provide a significant accelleration wrt the \emph{vanilla SGD} as expected, also the \emph{Polyak} momentm provide a quite comparable accelleration only reserved for the quadratic case according to the theoretical analysis.

Meanwhile, the actual \emph{convergence rates} of the \emph{primal} $\protect \mathcal{L}_2$-SVR formulations, i.e., the figure~\ref{fig:l2_svr_loss_history}, shows as they do not meet the theoretical expectations at the second line of the table~\ref{primal_svm_objectives_rates} since in this specific case, both the \emph{Polyak} and \emph{Nesterov} momentums does not provide an accelleration wrt the \emph{vanilla SGD} as expected.

\bigskip

The actual \emph{convergence rates} of the \emph{Lagrangian dual} formulations shows as they do not meet the theoretical expectations in the table~\ref{dual_svm_objectives_props}. The different \emph{convergence rate} is more highlighted in the linear case for lower regularization parameters $C$ but the situation is reversed as the latter grows. In the nonlinear settings, it depends on the kernel function, e.g., in the \emph{polynomial} case the convergence can become pathologically slower, meanwhile in the \emph{gaussian} or \emph{laplacian} case often it is better.

Moreover, from all the actual \emph{convergence rates} of the \emph{Lagrangian dual} formulations, it is evident that fitting the bias in an explicit way, i.e., by adding Lagrange multipliers to control the equality constraint, always causes slower converge of the \emph{AdaGrad} algorithm wrt the \emph{Lagrangian dual} of the problem where the bias term embedded into the Hessian matrix.

\bigskip

All the \emph{custom} implementations underperforms the others, i.e., \emph{liblinear}~\cite{fan2008liblinear}, \emph{libsvm}~\cite{chang2011libsvm} and \emph{cvxopt}~\cite{vandenberghe2010cvxopt} implementations, in terms of \emph{time} obviously in part due to the different core implementation languages, i.e., Python vs C, in part due to the different algorithm uses to solve the  optimization problem, e.g., the \emph{liblinear}~\cite{fan2008liblinear} implementation uses the \emph{Coordinate Gradient Descent} to sove the \emph{primal formulation} which minimizes one coordinate at a time.

Meanwhile, for what about the \emph{Wolfe dual} formulations, despite \emph{cvxopt}~\cite{vandenberghe2010cvxopt} underperforms the \emph{libsvm}~\cite{chang2011libsvm} implementation in terms of \emph{time}, since it is a general-purpose QP solver and it does not exploit the structure of the problem, the number of \emph{iterations} of the \emph{custom} SMO algorithm is always lower wrt that in \emph{libsvm}~\cite{chang2011libsvm}, probably due to the improvements described in~\cite{keerthi2001improvements, shevade1999improvements} for classification and regression respectively.

\bigskip

Finally, all the \emph{primal formulations} are suitable for potentially large linear training since the complexity of the model grows with the number of features or, more in general, when the number of examples $n$ is much larger than the number of features $m$, i.e., $n \gg m$.

Meanwhile, the \emph{dual formulations} are suitable in case the number of examples $n$ is less than the number of features $m$, i.e., $n < m$, since the complexity of the model is dominated by the number of examples. The \emph{Lagrangian} formulation never overperforms the \emph{Wolfe} one in our experiments, neither in terms of \emph{time} nor in terms of \emph{iterations}, but it is useful to highlight the complexity introduced by the dual formulation. Its training time complexity is more than quadratic with the number of samples which makes it hard to scale to large datasets. In this case, it could be useful to use the \emph{primal formulation} possibly after a nonlinear transformation of the instance vectors (if this should not be in the given space) using a low-rank kernel matrix approximation, i.e., NystrÃ¶m, before training.